
# DeepFake Detection using Vision Transformers

A video-based DeepFake detection system built using transfer learning with Vision Transformers (ViT-B/16). The model classifies videos as REAL or FAKE by extracting and aggregating temporal features from multiple frames.

---

## ğŸš€ Project Overview

This project focuses on detecting manipulated videos using deep learning and transfer learning techniques.

The objectives:

- Extract facial frames from videos
- Learn spatial features using pretrained Vision Transformers
- Aggregate temporal information across frames
- Classify videos as REAL or FAKE
- Evaluate model performance on unseen data

---

## ğŸ“‚ Dataset

- Real and Fake video samples
- Up to 200 REAL and 200 FAKE videos used
- 16 frames sampled per video
- 70% Train | 15% Validation | 15% Test split

---

## ğŸ§  Model Architecture

### ğŸ”¹ Feature Extraction
- Pretrained Vision Transformer (ViT-B/16)
- Frozen backbone (ImageNet weights)
- Frame-level embedding extraction (768-dim features)

### ğŸ”¹ Temporal Aggregation
- Frame averaging baseline
- Optional Transformer encoder head for sequence modeling

### ğŸ”¹ Classification
- Fully connected layer
- Sigmoid activation for binary classification
- BCEWithLogitsLoss

---

## âš™ï¸ Training Configuration

- Optimizer: AdamW
- Weight Decay Regularization
- 10 Training Epochs
- Batch Size: 1
- Sequence Length: 16 Frames
- Validation-based checkpointing

---

## ğŸ“Š Model Evaluation

- Precision
- Recall
- F1-Score
- Confusion Matrix
- Held-out Test Set Evaluation

---

## ğŸ–¥ Inference

The system includes:

- Single-video prediction function
- Confidence scoring
- Real-time frame visualization support

---

## ğŸ›  Tech Stack

Python | PyTorch | Torchvision | NumPy | OpenCV | Scikit-learn

---

## ğŸ“ˆ Key Outcomes

- Built an end-to-end video classification pipeline
- Implemented transfer learning for efficient training
- Designed modular Dataset & DataLoader architecture
- Enabled scalable DeepFake prediction workflow
